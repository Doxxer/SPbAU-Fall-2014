---
title: "Task1 (advertising) Тураев Тимур"
output: html_document
---
```{r, echo=FALSE}
library(lattice)
library(MASS)
library(e1071)
```

Загружаем данные:
```{r}
df <- read.csv("data/Advertising.csv", row.names = NULL)
df$X <- NULL
df.size <- nrow(df)
splom(~df)
```

Строим обучающую и тестовые выборки в отношении 2/1:
```{r}
indices.train <- sample(df.size, size=df.size*0.6666)
df.train <- df[indices.train, ]
df.test <- df[-indices.train, ]
```

Регрессия на обучающей выборке:
```{r}
df.lm.train <- lm(Sales ~ ., data=df.train)
summary(df.lm.train)
```

Предсказание по полученной модели для тестового куска:
```{r}
df.pred.test <- predict(df.lm.train, df.test)
```

Графички:
```{r}
panel.custom = function(...) { panel.xyplot(...); panel.loess(...); panel.lmline(...) }
xyplot(fitted(df.lm.train) ~ df.train$Sales, pch=19, panel=panel.custom)
xyplot(df.pred.test ~ df.test$Sales, pch=19, panel=panel.custom)
```

Видны ошибки на низких значениях Sales в той и другой выборке.

RSS:
```{r}
rss <- function(r) sqrt(sum(r^2) / length(r))
c(rss(df.lm.train$residuals),
  rss(df.pred.test - df.test$Sales))
```

Уберем незначительный признак из модели (это были газеты):
```{r}
df.lm.train.cut <- update(df.lm.train, . ~ . -Newspaper)
df.pred.test.cut <- predict(df.lm.train.cut, df.test)
c(rss(df.lm.train.cut$residuals),
  rss(df.pred.test.cut - df.test$Sales))
```
Модель стала вести себя немного лучше после удаления незначимого признака: уменьшились ошибки; стало быть он только мешал.

Поудаляем еще чего-нибудь, телевидение, например:
```{r}
df.lm.train.cut <- update(df.lm.train, . ~ . -TV)
df.pred.test.cut <- predict(df.lm.train.cut, df.test)
c(rss(df.lm.train.cut$residuals),
  rss(df.pred.test.cut - df.test$Sales))
```
Очень плохо.

Или радио:
```{r}
df.lm.train.cut <- update(df.lm.train, . ~ . -Radio)
df.pred.test.cut <- predict(df.lm.train.cut, df.test)
c(rss(df.lm.train.cut$residuals),
  rss(df.pred.test.cut - df.test$Sales))
```
Тоже плохо.

Или удалим вообще все признаки, оставив только сдвиг:
```{r}
df.lm.train.cut <- lm(Sales ~ 1, data=df.train)
df.pred.test.cut <- predict(df.lm.train.cut, df.test)
c(rss(df.lm.train.cut$residuals),
  rss(df.pred.test.cut - df.test$Sales))
```
И это тоже плохо.

Это и понятно, после удаления значимых признаков, модель ведет себя сильно хуже.

------------
```{r}
model <- function(f) {
  lm <- lm(f, data = df)
  print(summary(lm))
  lm <- stepAIC(lm)
  print(summary(lm))
  print(c(summary(lm)$sigma, AIC(lm)))
  lm
}
```

Посмотрим на разные модели. Выше функция, которая строить линейную регрессию, улучшает ее stepAIC, и выводи значение критерия и RSE.

Обычная
```{r}
lm.default <- model(Sales ~ .)
```
Из нее удалились газеты.

Теперь полиномиальная второй степени:
```{r}
lm.all <- model(Sales ~ poly(TV, Radio, degree = 2))
```
Модель явно улучшилась.

Все же удалим не очень значимый признак:
```{r}
lm.all2 <- model(Sales ~ (TV + Radio)^2 + I(TV^2))
```
Еще немного лучше.

```{r}
tune(lm, lm.default$call$formula, data = df, tunecontrol = tune.control(sampling = "fix"))
tune(lm, lm.all$call$formula, data = df, tunecontrol = tune.control(sampling = "fix"))
tune(lm, lm.all2$call$formula, data = df, tunecontrol = tune.control(sampling = "fix"))
xyplot(lm.all2$residuals ~ lm.all2$fitted.values)
```

Получается, что с введением монотонной нелинейности модель улучшилась, уменьшились ошибки и немного снизили излишнюю оптиимстичность в начале графика.

Посмотрим поведение на наших тренировочных выборках:
```{r}
lm.all2.train <- lm(Sales ~ (TV + Radio)^2 + I(TV^2), data=df.train)
df.pred2.test <- predict(lm.all2.train, df.test)
xyplot(lm.all2.train$fitted.values ~ df.train$Sales, pch=19, panel=panel.custom)
xyplot(df.pred2.test ~ df.test$Sales, pch=19, panel=panel.custom)
```

Круто, модель стала более лучшей.