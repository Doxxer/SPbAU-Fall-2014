---
title: "Seeds"
author: "Тураев Тимур"
date: "16 декабря 2014 г."
output:
  html_document:
    fig_caption: yes
    fig_height: 15
    fig_width: 15
    highlight: tango
    self_contained: no
    theme: spacelab
---
```{r echo=FALSE}
library(knitr)
library(lattice)
library(MASS)
library(e1071)
library(corrplot)
library(latticeExtra)
library(nnet)
```

Загружаем наши данные и переименовываем столбцы так, как указано в комментариях к данным.
```{r}
df <- read.table('data/seeds.txt')
names(df) <- c("area", "perimeter", "compactness", "length", "width", "asymmetry", "grooveLength", "variety")
df$variety <- as.factor(df$variety)
summary(df)
```

Построим всякие графики:
```{r}
splom(~df, data=df,
      upper.panel=function(x, y, ...) { panel.xyplot(x, y, ...); panel.loess(x, y, ..., col='red') },
      lower.panel=function(x, y, ...) { },
      pscale=0, varname.cex=1.4, par.settings=simpleTheme(pch=17, cex=0.2))
```

```{r}
corrplot.mixed(cor(subset(df, select=-variety)), tl.cex=1.5)
```

```{r}
marginal.plot(df)
```

Поделим выборку на обучающую и тестовую в отношении 2/1:
```{r}
train.idx <- sample(nrow(df), size = nrow(df) * 0.6666)
df.train <- df[train.idx, ]
df.test <- df[-train.idx, ]
```

### LDA

Установим функцию тестирования нашей модели (10-fold cross validation) и проверим test-train (выведем относительную ошибку)
```{r}
check.testTrain.lda <- function(f) {
  m.predicted <- predict(lda(f, data = df.train), df.test)
  print(table(m.predicted$class, df.test$variety))
  mean(m.predicted$class != df.test$variety)
}

check.lda = function(f) {
  print(lda(f, df))
  t <- tune(lda, f, data = df, predict.func = function(...) predict(...)$class)
  print(check.testTrain.lda(f))
  t
}

check.lda(variety ~ .)
```

Надо бы избавиться в модели от зависимых параметров.

Area и perimeter превращаются по формуле (в датасете она есть в описании данных) в compactness. Одно из измерений ядра тоже можно выкинуть. 

После всех проб и ошибок я выбрал такую модель, минимизирующую ошибку: оставим только длины зерна и "groove" (однажды я вообще получил полное совпадение)
```{r}
model <- variety ~ length + grooveLength
check.lda(model)
```

От запуска к запуску цифры все время разные, но раз модели предсказания почти одинаковые, почему бы не взять ту, которая проще :)

## Naive Bayes
Снова тестовая функция:
```{r}
check.testTrain.nb = function(f) {
  m.predicted <- predict(naiveBayes(f, data = df.train), df.test)
  print(table(m.predicted, df.test$variety))
  mean(m.predicted != df.test$variety)
}

check.nb = function(f) {
  print(naiveBayes(f, df))
  t <- tune(naiveBayes, f, data = df)
  print(check.testTrain.nb(f))
  t
}

check.nb(as.factor(variety) ~ .)
```

Попробуем ту же модель, что мы нашли для lda:
```{r}
model <- as.factor(variety) ~ length + grooveLength
check.nb(model)
```

Не очень. Оказалось, что такая чуть лучше, но все равно не кардинально улучшает модель, да и от запуска к запуску числа разные.
```{r}
model <- as.factor(variety) ~ area + grooveLength
check.nb(model)
```
Это, я думаю, происходит оттого, что данных не сказать что бы много, да и очень много зависимых параметров.

## Multinomial regression

Снова тестовые фукнции (как написано в письме (аж от 3 ноября :( ) trace выключен, там все сходится)
```{r}
check.testTrain.mr = function(f) {
  m.predicted <- predict(multinom(f, df.train, maxit = 2000, trace=FALSE), df.test)
  print(table(m.predicted, df.test$variety))
  mean(m.predicted != df.test$variety)
}

check.mr = function(f) {
  (mr <- multinom(f, df, maxit = 2000, trace=FALSE))
  t <- tune(multinom, f, data = df, maxit=2000, trace=FALSE)
  print(check.testTrain.mr(f))
  t
}

check.mr(as.factor(variety) ~ .)
```

Даже такая полная модель дала очень маленькую ошибку. Может попробовать поискать модель попроще с ошибкой той же степени?
Попробуем формулу LDA:
```{r}
model <- as.factor(variety) ~ length + grooveLength
check.mr(model)
```
Ошибка чуть больше.

Или из Байеса:
```{r}
model <- as.factor(variety) ~ area + grooveLength
check.mr(model)
```
Еще увеличилась.

В целом, ничего лучше начальной самой общей модели мне найти не удалось, LDA-формула оказалась ближе всех, да и модель вышла довольно простая.